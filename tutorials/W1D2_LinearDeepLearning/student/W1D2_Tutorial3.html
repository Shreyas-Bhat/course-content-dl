
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 3: Deep linear neural networks — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.jpg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W1D3_MultiLayerPerceptrons/chapter_title.html" rel="next" title="Multi Layer Perceptrons"/>
<link href="W1D2_Tutorial2.html" rel="prev" title="Tutorial 2: Learning Hyperparameters"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.jpg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/further_reading.html">
     Suggested further reading (TBD)
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 3: Deep linear neural networks
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-0-prelude">
   Section 0: Prelude
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-0-re-initialization">
     Coding Exercise 0: Re-initialization
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-deep-linear-neural-nets">
   Section 1: Deep Linear Neural Nets
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-representation-learning-intro">
     Video 1: Representation Learning (Intro)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-to-generate-and-visualize-training-samples-from-tree">
       Run to generate and visualize training samples from tree
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-training-the-deep-lnn">
     Interactive Demo 1: Training the deep LNN
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-sure-you-execute-this-cell-to-train-the-network-and-plot">
       Make sure you execute this cell to train the network and plot
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-sure-you-execute-this-cell-to-enable-the-widget">
       Make sure you execute this cell to enable the widget!
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-singular-value-decomposition-svd">
   Section 2: Singular Value Decomposition (SVD)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-singular-value-decomposition-svd">
     Video 2: Singular Value Decomposition (SVD)
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-svd">
     Coding Exercise 2: SVD
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
       Make sure you execute this cell to train the network and plot
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-representational-similarity-analysis-rsa">
   Section 3: Representational Similarity Analysis (RSA)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-representational-similarity-analysis-rsa">
     Video 3: Representational Similarity Analysis (RSA)
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-rsa">
     Coding Exercise 3: RSA
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-sure-you-execute-this-cell-to-enable-widgets">
       Make sure you execute this cell to enable widgets
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-illusory-correlations">
   Section 4: Illusory Correlations
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-illusory-correlations">
     Video 4: Illusory Correlations
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#demonstration-illusory-correlations">
     Demonstration: Illusory Correlations
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
       Make sure you execute this cell to train the network and plot
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-illusory-correlations-explained">
       Video 5: Illusory Correlations Explained
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-4-illusory-correlations">
     Exercise 4: Illusory Correlations
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
       Make sure you execute this cell to train the network and plot
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#wrap-up">
   Wrap up
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-outro">
     Video 6: Outro
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-linear-regression">
     Video 7: Linear Regression
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-1-linear-regression">
     Section 5.1: Linear Regression
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-2-vectorized-regression">
     Section 5.2: Vectorized regression
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-3-analytical-linear-regression">
     Section 5.3: Analytical Linear Regression
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-5-3-1-analytical-solution-to-lr">
       Coding Exercise 5.3.1: Analytical solution to LR
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#demonstration-linear-regression-vs-dlnn">
     Demonstration: Linear Regression vs. DLNN
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-3-deep-linear-neural-networks">
<h1>Tutorial 3: Deep linear neural networks<a class="headerlink" href="#tutorial-3-deep-linear-neural-networks" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 1, Day 2: Linear Deep Learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Saeed Salehi, Spiros Chavlis, Andrew Saxe</p>
<p><strong>Content reviewers:</strong> Polina Turishcheva</p>
<p><strong>Content editors:</strong> Anoop Kulkarni</p>
<p><strong>Production editors:</strong> Khalid Almubarak, Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p><hr class="docutils"/>
<p>#Tutorial Objectives</p>
<ul class="simple">
<li><p>Deep linear neural networks</p></li>
<li><p>Learning dynamics and singular value decomposition</p></li>
<li><p>Representational Similarity Analysis</p></li>
<li><p>Illusory correlations &amp; ethics</p></li>
</ul>
<p>Tutorial slides</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Tutorial slides</span>
<span class="c1"># you should link the slides for all tutorial videos here (we will store pdfs on osf)</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">'&lt;iframe src="https://docs.google.com/presentation/d/1ao5fJrxtQTMKaWXWALDtFjHeWFYl25M1btGfsS6TIs0/embed?start=false&amp;loop=false&amp;delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"&gt;&lt;/iframe&gt;'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">IntSlider</span><span class="p">,</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">interact_manual</span><span class="p">,</span> <span class="n">fixed</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">FloatLogSlider</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">Label</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interactive_output</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>

<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">'retina'</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Plotting functions</span>

<span class="k">def</span> <span class="nf">plot_x_y_hier_data</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">im2</span><span class="p">,</span> <span class="n">subplot_ratio</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="n">subplot_ratio</span><span class="p">)</span>
  <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"cool"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"cool"</span><span class="p">)</span>
  <span class="c1"># plt.suptitle("The whole dataset as imshow plot", y=1.02)</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Labels of all samples"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Features of all samples"</span><span class="p">)</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_x_y_hier_one</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">im2</span><span class="p">,</span> <span class="n">subplot_ratio</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="n">subplot_ratio</span><span class="p">)</span>
  <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"cool"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"cool"</span><span class="p">)</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Labels of a single sample"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Features of a single sample"</span><span class="p">)</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_tree_data</span><span class="p">(</span><span class="n">label_list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">feature_array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">new_feature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
  <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s1">'cyan'</span><span class="p">,</span> <span class="s1">'magenta'</span><span class="p">])</span>
  <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="n">n_labels</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="n">im1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_labels</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">feature_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">im2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">im2</span><span class="p">[</span><span class="n">im2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">feature_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'can_grow'</span><span class="p">,</span>
                    <span class="s1">'is_mammal'</span><span class="p">,</span>
                    <span class="s1">'has_leaves'</span><span class="p">,</span>
                    <span class="s1">'can_move'</span><span class="p">,</span>
                    <span class="s1">'has_trunk'</span><span class="p">,</span>
                    <span class="s1">'can_fly'</span><span class="p">,</span>
                    <span class="s1">'can_swim'</span><span class="p">,</span>
                    <span class="s1">'has_stem'</span><span class="p">,</span>
                    <span class="s1">'is_warmblooded'</span><span class="p">,</span>
                    <span class="s1">'can_flower'</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">im2</span> <span class="o">=</span> <span class="n">feature_array</span>
  <span class="k">if</span> <span class="n">label_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">label_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Goldfish'</span><span class="p">,</span> <span class="s1">'Tuna'</span><span class="p">,</span> <span class="s1">'Robin'</span><span class="p">,</span> <span class="s1">'Canary'</span><span class="p">,</span>
                  <span class="s1">'Rose'</span><span class="p">,</span> <span class="s1">'Daisy'</span><span class="p">,</span> <span class="s1">'Pine'</span><span class="p">,</span> <span class="s1">'Oak'</span><span class="p">]</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
  <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.35</span><span class="p">])</span>
  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">feature_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">implt</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">implt</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im2</span><span class="p">[:,</span> <span class="o">-</span><span class="n">n_features</span><span class="p">:],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
  <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">ax2</span><span class="p">)</span>
  <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s2">"right"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s2">"5%"</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">implt</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
  <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s1">'no'</span><span class="p">,</span> <span class="s1">'yes'</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Labels"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_labels</span><span class="p">))</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">label_list</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_labels</span><span class="p">))</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">label_list</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"</span><span class="si">{}</span><span class="s2"> random Features"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_features</span><span class="p">))</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_labels</span><span class="p">))</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">label_list</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">feature_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">feature_list</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="n">n_features</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="n">new_feature</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_loss</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Training loss (Mean Squared Error)"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"r"</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_loss_sv</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">sv_array</span><span class="p">):</span>
  <span class="n">n_sing_values</span> <span class="o">=</span> <span class="n">sv_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">sv_array</span> <span class="o">=</span> <span class="n">sv_array</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sv_array</span><span class="p">)</span>
  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"Set1"</span><span class="p">,</span> <span class="n">n_sing_values</span><span class="p">)</span>

  <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">plot1</span><span class="p">,</span> <span class="n">plot2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
  <span class="n">plot1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Training loss (Mean Squared Error)"</span><span class="p">)</span>
  <span class="n">plot1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>

  <span class="n">plot2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Evolution of singular values (modes)"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">):</span>
    <span class="n">plot2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sv_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
  <span class="n">plot2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_loss_sv_twin</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">sv_array</span><span class="p">):</span>
  <span class="n">n_sing_values</span> <span class="o">=</span> <span class="n">sv_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">sv_array</span> <span class="o">=</span> <span class="n">sv_array</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sv_array</span><span class="p">)</span>
  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"winter"</span><span class="p">,</span> <span class="n">n_sing_values</span><span class="p">)</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Learning Dynamics"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Mean Squared Error"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Singular values (modes)"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">):</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sv_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

  <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_ills_sv_twin</span><span class="p">(</span><span class="n">ill_array</span><span class="p">,</span> <span class="n">sv_array</span><span class="p">,</span> <span class="n">ill_label</span><span class="p">):</span>
  <span class="n">n_sing_values</span> <span class="o">=</span> <span class="n">sv_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">sv_array</span> <span class="o">=</span> <span class="n">sv_array</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sv_array</span><span class="p">)</span>
  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"winter"</span><span class="p">,</span> <span class="n">n_sing_values</span><span class="p">)</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Network training and the Illusory Correlations"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ill_label</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ill_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
  <span class="c1"># ax1.set_yticks([-1, 0, 1])</span>
  <span class="c1"># ax1.set_yticklabels(['False', 'Not sure', 'True'])</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Singular values (modes)"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">):</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sv_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

  <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_loss_sv_rsm</span><span class="p">(</span><span class="n">loss_array</span><span class="p">,</span> <span class="n">sv_array</span><span class="p">,</span> <span class="n">rsm_array</span><span class="p">,</span> <span class="n">i_ep</span><span class="p">):</span>
  <span class="n">n_ep</span> <span class="o">=</span> <span class="n">loss_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">rsm_array</span> <span class="o">=</span> <span class="n">rsm_array</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">rsm_array</span><span class="p">)</span>
  <span class="n">sv_array</span> <span class="o">=</span> <span class="n">sv_array</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sv_array</span><span class="p">)</span>

  <span class="n">n_sing_values</span> <span class="o">=</span> <span class="n">sv_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"winter"</span><span class="p">,</span> <span class="n">n_sing_values</span><span class="p">)</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

  <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">tick_right</span><span class="p">()</span>
  <span class="n">implot</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">rsm_array</span><span class="p">[</span><span class="n">i_ep</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"Purples"</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
  <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">ax0</span><span class="p">)</span>
  <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s2">"right"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s2">"5%"</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">implot</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
  <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Similarity'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"RSM at epoch </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i_ep</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
  <span class="c1"># ax0.set_axis_off()</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">))</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">item_names</span><span class="p">)</span>
  <span class="c1"># ax0.set_xticks([])</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">))</span>
  <span class="n">ax0</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">item_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>

  <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Learning Dynamics"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Mean Squared Error"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">"in"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_ep</span><span class="p">),</span> <span class="n">loss_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">i_ep</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">i_ep</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'m'</span><span class="p">)</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Singular values"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">"in"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sing_values</span><span class="p">):</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_ep</span><span class="p">),</span> <span class="n">sv_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_ep</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_ep</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper functions</span>

<span class="k">def</span> <span class="nf">build_tree</span><span class="p">(</span><span class="n">n_levels</span><span class="p">,</span> <span class="n">n_branches</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">to_np_array</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">"""Builds a tree</span>
<span class="sd">  """</span>
  <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">probability</span> <span class="o">&lt;=</span> <span class="mf">1.0</span>

  <span class="n">tree</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_levels</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">n_branches</span><span class="o">**</span><span class="n">i</span><span class="p">))</span>

  <span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">probability</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">])</span>

  <span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
  <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="o">//</span><span class="n">n_branches</span><span class="p">):</span>
    <span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">n_branches</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">to_np_array</span><span class="p">:</span>
    <span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">])</span>
    <span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">])</span>
    <span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">tree</span>


<span class="k">def</span> <span class="nf">sample_from_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
  <span class="sd">""" Generates n samples from a tree</span>
<span class="sd">  """</span>
  <span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">])</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="nb">max</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"level"</span><span class="p">])]</span>
  <span class="n">n_items</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n_items</span><span class="p">))</span>
  <span class="n">rand_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">]))</span>
  <span class="n">flip_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">samp</span> <span class="o">=</span> <span class="p">(</span><span class="n">rand_temp</span> <span class="o">&gt;</span> <span class="n">flip_temp</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_items</span><span class="p">):</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">items</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">prop</span> <span class="o">=</span> <span class="n">samp</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>
    <span class="k">while</span> <span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">j</span> <span class="o">=</span> <span class="n">tree</span><span class="p">[</span><span class="s2">"parent"</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
      <span class="n">prop</span> <span class="o">=</span> <span class="n">prop</span> <span class="o">*</span> <span class="n">samp</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prop</span><span class="o">.</span><span class="n">T</span>
  <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">generate_hsd</span><span class="p">():</span>
  <span class="c1"># building the tree</span>
  <span class="n">n_branches</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 branches at each node</span>
  <span class="n">probability</span> <span class="o">=</span> <span class="mf">.15</span>  <span class="c1"># flipping probability</span>
  <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># number of levels (depth of tree)</span>
  <span class="n">tree</span> <span class="o">=</span> <span class="n">build_tree</span><span class="p">(</span><span class="n">n_levels</span><span class="p">,</span> <span class="n">n_branches</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">to_np_array</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">tree</span><span class="p">[</span><span class="s2">"pflip"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>
  <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># Sample this many features</span>

  <span class="n">tree_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_branches</span><span class="o">**</span><span class="n">n_levels</span><span class="p">)</span>
  <span class="n">tree_features</span> <span class="o">=</span> <span class="n">sample_from_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
  <span class="k">return</span> <span class="n">tree_labels</span><span class="p">,</span> <span class="n">tree_features</span>


<span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
  <span class="sd">"""Analytical Linear regression</span>

<span class="sd">  """</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
  <span class="n">M</span><span class="p">,</span> <span class="n">Dx</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">N</span><span class="p">,</span> <span class="n">Dy</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">assert</span> <span class="n">Dx</span> <span class="o">==</span> <span class="n">Dy</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">W</span>


<span class="k">def</span> <span class="nf">add_feature</span><span class="p">(</span><span class="n">existing_features</span><span class="p">,</span> <span class="n">new_feature</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_features</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_feature</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
  <span class="n">new_feature</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">new_feature</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
  <span class="c1"># return np.hstack((tree_features, new_feature*2-1))</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">tree_features</span><span class="p">,</span> <span class="n">new_feature</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">net_svd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
  <span class="sd">"""Performs a Singular Value Decomposition on a given model weights</span>

<span class="sd">  Args:</span>
<span class="sd">    model (torch.nn.Module): neural network model</span>
<span class="sd">    in_dim (int): the input dimension of the model</span>

<span class="sd">  Returns:</span>
<span class="sd">    U, Σ, V (Tensors): Orthogonal, diagonal, and orthogonal matrices</span>
<span class="sd">  """</span>
  <span class="n">W_tot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">in_dim</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">W_tot</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">@</span> <span class="n">W_tot</span>
  <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">W_tot</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span>


<span class="k">def</span> <span class="nf">net_rsm</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
  <span class="sd">"""Calculates the Representational Similarity Matrix</span>

<span class="sd">  Arg:</span>
<span class="sd">    h (torch.Tensor): activity of a hidden layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor): Representational Similarity Matrix</span>
<span class="sd">  """</span>
  <span class="n">rsm</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span>
  <span class="k">return</span> <span class="n">rsm</span>


<span class="k">def</span> <span class="nf">initializer_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
  <span class="sd">"""(in-place) Re-initialization of weights</span>

<span class="sd">  Args:</span>
<span class="sd">    model (torch.nn.Module): PyTorch neural net model</span>
<span class="sd">    gamma (float): initialization scale</span>
<span class="sd">  """</span>
  <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">n_out</span><span class="p">,</span> <span class="n">n_in</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test_initializer_ex</span><span class="p">():</span>
  <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7337</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">ex_initializer_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">if</span> <span class="o">-</span><span class="mf">1e-5</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">std</span> <span class="o">-</span> <span class="mf">0.01</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1e-5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Well done! Seems to be correct!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Please double check your implementation!"</span><span class="p">)</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Faulty Implementation!"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test_net_svd_ex</span><span class="p">():</span>
  <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7337</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">U_ex</span><span class="p">,</span> <span class="n">Σ_ex</span><span class="p">,</span> <span class="n">V_ex</span> <span class="o">=</span> <span class="n">ex_net_svd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">net_svd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">U_ex</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">U</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span> <span class="ow">and</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">Σ_ex</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">Σ</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span> <span class="ow">and</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">V_ex</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">V</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))):</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Well done! Seems to be correct!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Please double check your implementation!"</span><span class="p">)</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Faulty Implementation!"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test_net_rsm_ex</span><span class="p">():</span>
  <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7337</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">y_ex</span> <span class="o">=</span> <span class="n">ex_net_rsm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">y_ex</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))):</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Well done! Seems to be correct!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"Please double check your implementation!"</span><span class="p">)</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Faulty Implementation!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Set random seed</span>

<span class="c1">#@markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># for DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This colab notebook is GPU free!</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-0-prelude">
<h1>Section 0: Prelude<a class="headerlink" href="#section-0-prelude" title="Permalink to this headline">¶</a></h1>
<p>Throughout this tutorial, we will use a linear neural net with a single hidden layer. We have also excluded <code class="docutils literal notranslate"><span class="pre">bias</span></code> from the layers.</p>
<p><strong>important to remember</strong>: The forward loop returns the hidden activation, besides the network output (prediction). we will need it in section 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LNNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""A Linear Neural Net with one hidden layer</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">      in_dim (int): input dimension</span>
<span class="sd">      out_dim (int): ouput dimension</span>
<span class="sd">      hid_dim (int): hidden dimension</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_hid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hid_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">      x (torch.Tensor): input tensor</span>
<span class="sd">    """</span>
    <span class="n">hid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_hid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># hidden activity</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hid_out</span><span class="p">(</span><span class="n">hid</span><span class="p">)</span>  <span class="c1"># output (prediction)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">hid</span>
</pre></div>
</div>
</div>
</div>
<p>Other than <code class="docutils literal notranslate"><span class="pre">net_svd</span></code> and <code class="docutils literal notranslate"><span class="pre">net_rsm</span></code> functions, the training loop should be mostly familiar to you. We will define these functions in the coming sections.</p>
<p><strong>important</strong>: Please note that the two functions are part of inner training loop and are therefore executed and recorded at every iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">illusory_i</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""Training function</span>

<span class="sd">  Args:</span>
<span class="sd">    model (torch nn.Module): the neural network</span>
<span class="sd">    inputs (torch.Tensor): features (input) with shape `[batch_size, input_dim]`</span>
<span class="sd">    targets (torch.Tensor): targets (labels) with shape `[batch_size, output_dim]`</span>
<span class="sd">    n_epochs (int): number of training epochs (iterations)</span>
<span class="sd">    lr (float): learning rate</span>
<span class="sd">    illusory_i (int): index of illusory feature</span>

<span class="sd">  Returns:</span>
<span class="sd">    np.ndarray: record (evolution) of training loss</span>
<span class="sd">    np.ndarray: record (evolution) of singular values (dynamic modes)</span>
<span class="sd">    np.ndarray: record (evolution) of representational similarity matrices</span>
<span class="sd">    np.ndarray: record of network prediction for the last feature</span>
<span class="sd">  """</span>
  <span class="n">in_dim</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>  <span class="c1"># loss records</span>
  <span class="n">modes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">))</span>  <span class="c1"># singular values (modes) records</span>
  <span class="n">rs_mats</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># representational similarity matrices</span>
  <span class="n">illusions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>  <span class="c1"># prediction for the given feature</span>

  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
  <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Section 2 Singular value decomposition</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">net_svd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span>

    <span class="c1"># Section 3 calculating representational similarity matrix</span>
    <span class="n">RSM</span> <span class="o">=</span> <span class="n">net_rsm</span><span class="p">(</span><span class="n">hiddens</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

    <span class="c1"># Section 4 network prediction of illusory_i inputs for the last feature</span>
    <span class="n">pred_ij</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">illusory_i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># logging (recordings)</span>
    <span class="n">losses</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">modes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Σ</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">rs_mats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RSM</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">illusions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_ij</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rs_mats</span><span class="p">),</span> <span class="n">illusions</span>
</pre></div>
</div>
</div>
</div>
<p>We also need take over the initialization of the weights. In PyTorch, <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html"><code class="docutils literal notranslate"><span class="pre">nn.init</span></code></a> provides us with the functions to initialize tensors from a given distribution.</p>
<p><strong>important</strong>: Since we need to make sure the plots are correct, so the tutorial message is delivered, we test your exercise implementations but we will not use them for the plots and training.</p>
<div class="section" id="coding-exercise-0-re-initialization">
<h2>Coding Exercise 0: Re-initialization<a class="headerlink" href="#coding-exercise-0-re-initialization" title="Permalink to this headline">¶</a></h2>
<p>Complete the function <code class="docutils literal notranslate"><span class="pre">ex_initializer_</span></code>, such that the weights are sampled from the following distribution:</p>
<div class="math notranslate nohighlight">
\[\mathcal{N}\left(\mu=0, ~~\sigma=\gamma \sqrt{\dfrac{1}{n_{in} + n_{out}}} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is the initialization scale, <span class="math notranslate nohighlight">\(n_{in}\)</span> and <span class="math notranslate nohighlight">\(n_{out}\)</span> are respectively input and output dimensions of the layer. the Underscore (“_”) in <code class="docutils literal notranslate"><span class="pre">ex_initializer_</span></code> and other functions, denotes “<a class="reference external" href="https://discuss.pytorch.org/t/what-is-in-place-operation/16244/2">in-place</a>” operation.</p>
<p><strong>important note</strong>: since we did not include bias in the layers, the <code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code> would only return the weights in each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ex_initializer_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
  <span class="sd">"""(in-place) Re-initialization of weights</span>

<span class="sd">  Args:</span>
<span class="sd">    model (torch.nn.Module): PyTorch neural net model</span>
<span class="sd">    gamma (float): initialization scale</span>
<span class="sd">  """</span>
  <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">n_out</span><span class="p">,</span> <span class="n">n_in</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1">#################################################</span>
    <span class="c1">## Define the standard deviation (sigma) for the normal distribution</span>
    <span class="c1"># as given in the equation above</span>
    <span class="c1"># Complete the function and remove or comment the line below</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Function `ex_initializer_`"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>


<span class="c1">## uncomment and run</span>
<span class="c1"># test_initializer_ex()</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial3_Solution_8af375c3.py"><em>Click for solution</em></a></p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-deep-linear-neural-nets">
<h1>Section 1: Deep Linear Neural Nets<a class="headerlink" href="#section-1-deep-linear-neural-nets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-representation-learning-intro">
<h2>Video 1: Representation Learning (Intro)<a class="headerlink" href="#video-1-representation-learning-intro" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>So far, depth just seems to slow down the learning. And we know that a single nonlinear hidden layer (given enough number of neurons and infinite training samples) has the potential to approximate any function. So it seems fair to ask: <strong>What is depth good for</strong>?</p>
<p>One reason can be that shallow nonlinear neural networks hardly meet their true potential in practice. In the contrast, deep neural nets are often surprisingly powerful in learning complex functions without sacrificing generalization. A core intuition behind deep learning is that deep nets derive their power through learning internal representations. How does this work? To address representation learning, we have to go beyond the 1D chain.</p>
<p>For this and the next couple of exercises, we use syntactically generated hierarchically structured data through a <em>branching diffusion process</em> (see <a class="reference external" href="https://www.pnas.org/content/pnas/suppl/2019/05/16/1820226116.DCSupplemental/pnas.1820226116.sapp.pdf">this reference</a> for more details).</p>
<center><img alt="Simple nn graph" src="https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/tree.png" width="600"/></center>
<center> hierarchically structured data (a tree) </center>
<p>The inputs to the network are labels (i.e. names), while the outputs are the features (i.e. attributes). For example, for the label “Goldfish”, the network has to learn all the (artificially created) features, such as “<em>can swim</em>”, “<em>is cold-blooded</em>”, “<em>has fins</em>”, and more. Given that we are training on hierarchically structured data, network could also learn the tree structure, that Goldfish and Tuna have rather similar features, and Robin has more in common with Tuna, compared to Rose.</p>
<div class="section" id="run-to-generate-and-visualize-training-samples-from-tree">
<h3>Run to generate and visualize training samples from tree<a class="headerlink" href="#run-to-generate-and-visualize-training-samples-from-tree" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Run to generate and visualize training samples from tree</span>

<span class="n">tree_labels</span><span class="p">,</span> <span class="n">tree_features</span> <span class="o">=</span> <span class="n">generate_hsd</span><span class="p">()</span>

<span class="c1"># convert (cast) data from np.ndarray to torch.Tensor</span>
<span class="n">label_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">feature_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_features</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">item_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Goldfish'</span><span class="p">,</span> <span class="s1">'Tuna'</span><span class="p">,</span> <span class="s1">'Robin'</span><span class="p">,</span> <span class="s1">'Canary'</span><span class="p">,</span>
              <span class="s1">'Rose'</span><span class="p">,</span> <span class="s1">'Daisy'</span><span class="p">,</span> <span class="s1">'Pine'</span><span class="p">,</span> <span class="s1">'Oak'</span><span class="p">]</span>
<span class="n">plot_tree_data</span><span class="p">()</span>

<span class="c1"># dimensions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"---------------------------------------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input Dimension: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output Dimension: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of samples: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>To continue this tutorial, it is vital to understand the premise of our training data and what the task is. Therefore, please take your time to discuss them with your pod.</p>
<center><img alt="neural net" src="https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/neural_net.png" width="600"/></center>
<center> The neural network used for this tutorial </center></div>
</div>
<div class="section" id="interactive-demo-1-training-the-deep-lnn">
<h2>Interactive Demo 1: Training the deep LNN<a class="headerlink" href="#interactive-demo-1-training-the-deep-lnn" title="Permalink to this headline">¶</a></h2>
<p>Training a neural net on our data is straight forward. But before executing the next cell, remember the training loss curve from previous tutorial.</p>
<div class="section" id="make-sure-you-execute-this-cell-to-train-the-network-and-plot">
<h3>Make sure you execute this cell to train the network and plot<a class="headerlink" href="#make-sure-you-execute-this-cell-to-train-the-network-and-plot" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to train the network and plot</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

<span class="c1"># model instantiation</span>
<span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

<span class="c1"># weights re-initialization</span>
<span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">losses</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                  <span class="n">label_tensor</span><span class="p">,</span>
                  <span class="n">feature_tensor</span><span class="p">,</span>
                  <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                  <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># plotting</span>
<span class="n">plot_loss</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Question</strong>: Why haven’t we seen these “bumps” in training before? And should we look for them in the future? What do these bumps mean?</p>
<p>Recall from previous tutorial, that we are always interested in learning rate (<span class="math notranslate nohighlight">\(\eta\)</span>) and initialization (<span class="math notranslate nohighlight">\(\gamma\)</span>) that would give us the fastest but yet stable (reliable) convergence. Try finding the optimal <span class="math notranslate nohighlight">\(\eta\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span> using the following widgets. More specifically, try large <span class="math notranslate nohighlight">\(\gamma\)</span> and see if we can recover the bumps by tuning the <span class="math notranslate nohighlight">\(\eta\)</span>.</p>
</div>
<div class="section" id="make-sure-you-execute-this-cell-to-enable-the-widget">
<h3>Make sure you execute this cell to enable the widget!<a class="headerlink" href="#make-sure-you-execute-this-cell-to-enable-the-widget" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to enable the widget!</span>

<span class="k">def</span> <span class="nf">loss_lr_init</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
  <span class="sd">"""Trains and plots the loss evolution given lr and initialization</span>
<span class="sd">  Args:</span>
<span class="sd">    lr (float): learning rate</span>
<span class="sd">    gamma (float): initialization scale</span>
<span class="sd">  """</span>
  <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
  <span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
  <span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
  <span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

  <span class="c1"># model instantiation</span>
  <span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

  <span class="c1"># weights re-initialization</span>
  <span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

  <span class="n">losses</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                    <span class="n">label_tensor</span><span class="p">,</span>
                    <span class="n">feature_tensor</span><span class="p">,</span>
                    <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                    <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

  <span class="n">plot_loss</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">interact</span><span class="p">(</span><span class="n">loss_lr_init</span><span class="p">,</span>
             <span class="n">lr</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">200.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span>
                             <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">readout_format</span><span class="o">=</span><span class="s1">'.1f'</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'η'</span><span class="p">),</span>
             <span class="n">epochs</span> <span class="o">=</span> <span class="n">fixed</span><span class="p">(</span><span class="mi">250</span><span class="p">),</span>
             <span class="n">gamma</span> <span class="o">=</span> <span class="n">FloatLogSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">15</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                             <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'γ'</span><span class="p">),</span>
             <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-singular-value-decomposition-svd">
<h1>Section 2: Singular Value Decomposition (SVD)<a class="headerlink" href="#section-2-singular-value-decomposition-svd" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-2-singular-value-decomposition-svd">
<h2>Video 2: Singular Value Decomposition (SVD)<a class="headerlink" href="#video-2-singular-value-decomposition-svd" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>In this section, we intend to study the learning (training) dynamics we just saw. First, we should know that a linear neural network is performing sequential matrix multiplications, which can be simplified to:</p>
<div class="amsmath math notranslate nohighlight" id="equation-379f9ec0-d1df-4603-914a-e4af20d3c1e6">
<span class="eqno">(4)<a class="headerlink" href="#equation-379f9ec0-d1df-4603-914a-e4af20d3c1e6" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbf{y} &amp;= \mathbf{W}_{L}~\mathbf{W}_{L-1}~\dots~\mathbf{W}_{1} ~ \mathbf{x} \\
 &amp;= (\prod_{i=1}^{L}{\mathbf{W}_{i}}) ~ \mathbf{x} \\
 &amp;= \mathbf{W}_{tot} ~ \mathbf{x}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> denotes the number of layers in our network.</p>
<p>Learning through gradient descent seems very alike to the evolution of a dynamic system. They both are described by a set of differential equations. Dynamical systems often have a “time-constant” which describes the rate of change, similar to the learning rate, only instead of time, gradient descent evolves through epochs.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1312.6120">Saxe et al. (2013)</a> showed that to analyze and to understanding the nonlinear learning dynamics of a deep LNN, we can use <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition (SVD)</a> to decompose the <span class="math notranslate nohighlight">\(\mathbf{W}_{tot}\)</span> into orthogonal vectors, where orthogonality of the vectors would ensure their “individuality (independence)”. This means we can break a deep wide LNN into multiple deep narrow LNN, so their activity is untangled from each other.</p>
<br/>
<p><strong>A Quick intro to SVD</strong></p>
<p>Any real-valued matix <span class="math notranslate nohighlight">\(A\)</span> (yes, ANY) can be decomposed (factorized) to 3 matrices:</p>
<div class="amsmath math notranslate nohighlight" id="equation-082af91b-b24c-4f98-a113-30699a03ed5b">
<span class="eqno">(5)<a class="headerlink" href="#equation-082af91b-b24c-4f98-a113-30699a03ed5b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{A} = \mathbf{U} \mathbf{Σ} \mathbf{V}^{\top}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span> is an orthogonal matrix, <span class="math notranslate nohighlight">\(\Sigma\)</span> is a diagonal matrix, and <span class="math notranslate nohighlight">\(V\)</span> is again an orthogonal matrix. The diagonal elements of <span class="math notranslate nohighlight">\(\Sigma\)</span> are called <strong>singular values</strong>.</p>
<p>The main difference between SVD and EigenValue Decomposition (EVD), is that EVD requires <span class="math notranslate nohighlight">\(A\)</span> to be squared and does not guarantee the eigenvectors to be orthogonal. For the complex-valued matrix <span class="math notranslate nohighlight">\(A\)</span>, the factorization changes to <span class="math notranslate nohighlight">\(A = UΣV^*\)</span> and <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are unitary matrices.</p>
<p>We strongly recommend the <a class="reference external" href="https://www.youtube.com/watch?v=mBcLRGuAFUk">Singular Value Decomposition (the SVD)</a> by the amazing <a class="reference external" href="http://www-math.mit.edu/~gs/">Gilbert Strang</a> if you would like to learn more.</p>
</div>
<div class="section" id="coding-exercise-2-svd">
<h2>Coding Exercise 2: SVD<a class="headerlink" href="#coding-exercise-2-svd" title="Permalink to this headline">¶</a></h2>
<p>The goal is to perform the SVD on <span class="math notranslate nohighlight">\(\mathbf{W}_{tot}\)</span> in every epoch, and record the singular values (modes) during the training.</p>
<p>Complete the function <code class="docutils literal notranslate"><span class="pre">ex_net_svd</span></code>, by first calculating the <span class="math notranslate nohighlight">\(\mathbf{W}_{tot} = \prod_{i=1}^{L}{\mathbf{W}_{i}}\)</span> and finally performing SVD on the <span class="math notranslate nohighlight">\(\mathbf{W}_{tot}\)</span>. Please use the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.svd.html"><code class="docutils literal notranslate"><span class="pre">torch.svd</span></code></a> instead of NumPy <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">np.linalg.svd</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ex_net_svd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
  <span class="sd">"""Performs a Singular Value Decomposition on a given model weights</span>

<span class="sd">  Args:</span>
<span class="sd">    model (torch.nn.Module): neural network model</span>
<span class="sd">    in_dim (int): the input dimension of the model</span>

<span class="sd">  Returns:</span>
<span class="sd">    U, Σ, V (Tensors): Orthogonal, diagonal, and orthogonal matrices</span>
<span class="sd">  """</span>
  <span class="n">W_tot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">in_dim</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="c1">#################################################</span>
    <span class="c1">## Calculate the W_tot by multiplication of all weights</span>
    <span class="c1"># and then perform SVD on the W_tot using pytorch's `torch.svd`</span>
    <span class="c1"># Remember that weights need to be `.detach()` from the graph</span>
    <span class="c1"># Complete the function and remove or comment the line below</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Function `ex_net_svd`"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="n">W_tot</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">Σ</span><span class="p">,</span> <span class="n">V</span>


<span class="c1"># # Uncomment and run</span>
<span class="c1"># test_net_svd_ex()</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial3_Solution_891d558a.py"><em>Click for solution</em></a></p>
<div class="section" id="id1">
<h3>Make sure you execute this cell to train the network and plot<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to train the network and plot</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

<span class="c1"># model instantiation</span>
<span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

<span class="c1"># weights re-initialization</span>
<span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                          <span class="n">label_tensor</span><span class="p">,</span>
                          <span class="n">feature_tensor</span><span class="p">,</span>
                          <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                          <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">plot_loss_sv_twin</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Questions</strong>: In EigenValue decomposition, the amount of variance explained by eigenvectors is proportional to the corresponding eigenvalues. What about the SVD? We see that the gradient descent guides the network to first learn the features that carry more information (have higher singular value)!</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-representational-similarity-analysis-rsa">
<h1>Section 3: Representational Similarity Analysis (RSA)<a class="headerlink" href="#section-3-representational-similarity-analysis-rsa" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-3-representational-similarity-analysis-rsa">
<h2>Video 3: Representational Similarity Analysis (RSA)<a class="headerlink" href="#video-3-representational-similarity-analysis-rsa" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>The previous section ended with an interesting remark. SVD helped to break our deep “wide” linear neural net into 8 deep “narrow” linear neural nets. Although the naive interpretation could be that each narrow net is learning an item (e.g. Goldfish), the structure of modes evolution implies something deeper. The first narrow net (highest singular value) converges fastest, while the last four narrow nets, converge almost simultaneously and have the smallest singular values. Maybe, one narrow net is learning the difference between “living things” and “objects”, while another narrow net is learning the difference between Fish and Birds. And the narrow nets that are learning more informative distinction are trained first. So, how could we check this hypothesis?</p>
<p>Representational Similarity Analysis (RSA) is an approach that could help us understand the internal representation of our network. The main idea is that the activity of hidden units (neurons) in the network must be similar when the network is presented with similar input. For our dataset (hierarchically structured data), we expect the activity of neurons in the hidden layer to be more similar for Tuna and Canary, and less similar for Tuna and Oak.</p>
<p>If we perform RSA in every training iteration, we may be able to see whether the narrow nets are learning the representations or our hypothesis is empty.</p>
</div>
<div class="section" id="coding-exercise-3-rsa">
<h2>Coding Exercise 3: RSA<a class="headerlink" href="#coding-exercise-3-rsa" title="Permalink to this headline">¶</a></h2>
<p>The task is simple. We would need to measure the similarity between hidden layer activities <span class="math notranslate nohighlight">\(~\mathbf{h} =  \mathbf{x} ~\mathbf{W_1}\)</span>) for every input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>For similarity measure, we can use the good old dot (scalar) product, which is also called cosine similarity. For calculating the dot product between multiple vectors (which would be our case), you can simply use matrix multiplication. Therefore the Representational Similarity Matrix for multiple-input (batch) activity could be calculated as follow:</p>
<div class="math notranslate nohighlight">
\[ RSM = \mathbf{H} ~ \mathbf{H}^{\top} \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{H} = \mathbf{X} ~ \mathbf{W_1}\)</span> is the activity of hidden neurons for a given batch <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>If we perform RSA in every iteration, we could also see the evolution of representation learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ex_net_rsm</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
  <span class="sd">"""Calculates the Representational Similarity Matrix</span>

<span class="sd">  Arg:</span>
<span class="sd">    h (torch.Tensor): activity of a hidden layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor): Representational Similarity Matrix</span>
<span class="sd">  """</span>
  <span class="c1">#################################################</span>
  <span class="c1">## Calculate the Representational Similarity Matrix</span>
  <span class="c1"># Complete the function and remove or comment the line below</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Function `ex_net_rsm`"</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="n">rsm</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">return</span> <span class="n">rsm</span>


<span class="c1"># # Uncomment and run</span>
<span class="c1"># test_net_rsm_ex()</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial3_Solution_05426d8c.py"><em>Click for solution</em></a></p>
<p>Now we can train the model while recording the losses, modes, and RSMs at every iteration. First, use the epoch slider to explore the evolution of RSM without changing default lr (<span class="math notranslate nohighlight">\(\eta\)</span>) and initialization (<span class="math notranslate nohighlight">\(\gamma\)</span>). Then, as we did before, set <span class="math notranslate nohighlight">\(\eta\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span> to larger values to see whether you can retrieve the sequential structured learning of representations.</p>
<div class="section" id="make-sure-you-execute-this-cell-to-enable-widgets">
<h3>Make sure you execute this cell to enable widgets<a class="headerlink" href="#make-sure-you-execute-this-cell-to-enable-widgets" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to enable widgets</span>

<span class="k">def</span> <span class="nf">loss_svd_rsm_lr_gamma</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">i_ep</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Args:</span>
<span class="sd">    lr (float): learning rate</span>
<span class="sd">    gamma (float): initialization scale</span>
<span class="sd">    i_ep (int): which epoch to show</span>

<span class="sd">  """</span>
  <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
  <span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
  <span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
  <span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

  <span class="c1"># model instantiation</span>
  <span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

  <span class="c1"># weights re-initialization</span>
  <span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

  <span class="c1"># training</span>
  <span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">rsms</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                                <span class="n">label_tensor</span><span class="p">,</span>
                                <span class="n">feature_tensor</span><span class="p">,</span>
                                <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                                <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
  <span class="n">plot_loss_sv_rsm</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">rsms</span><span class="p">,</span> <span class="n">i_ep</span><span class="p">)</span>

<span class="n">i_ep_slider</span> <span class="o">=</span> <span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">241</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">61</span><span class="p">,</span>
                        <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'Epoch'</span><span class="p">,</span>
                        <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">'630px'</span><span class="p">))</span>

<span class="n">lr_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">200.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span>
                        <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">readout_format</span><span class="o">=</span><span class="s1">'.1f'</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'η'</span><span class="p">)</span>

<span class="n">gamma_slider</span> <span class="o">=</span> <span class="n">FloatLogSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">15</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                             <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'γ'</span><span class="p">)</span>

<span class="n">widgets_ui</span> <span class="o">=</span> <span class="n">VBox</span><span class="p">([</span><span class="n">lr_slider</span><span class="p">,</span> <span class="n">gamma_slider</span><span class="p">,</span> <span class="n">i_ep_slider</span><span class="p">])</span>

<span class="n">widgets_out</span> <span class="o">=</span> <span class="n">interactive_output</span><span class="p">(</span><span class="n">loss_svd_rsm_lr_gamma</span><span class="p">,</span>
                                 <span class="p">{</span><span class="s1">'lr'</span><span class="p">:</span> <span class="n">lr_slider</span><span class="p">,</span>
                                  <span class="s1">'gamma'</span><span class="p">:</span> <span class="n">gamma_slider</span><span class="p">,</span>
                                  <span class="s1">'i_ep'</span><span class="p">:</span> <span class="n">i_ep_slider</span><span class="p">})</span>

<span class="n">display</span><span class="p">(</span><span class="n">widgets_ui</span><span class="p">,</span> <span class="n">widgets_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s take a moment to analyze this more. A deep neural net is learning the representations, rather than a naive mapping (look-up table). This is thought to be the reason for deep neural nets supreme generalization and transfer learning ability. Unsurprisingly, neural nets with no hidden layer are incapable of representation learning, even with extremely small initialization.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-illusory-correlations">
<h1>Section 4: Illusory Correlations<a class="headerlink" href="#section-4-illusory-correlations" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-4-illusory-correlations">
<h2>Video 4: Illusory Correlations<a class="headerlink" href="#video-4-illusory-correlations" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>So far, everything looks great and all our trainings are successful (training loss converging to zero), and very fast. We even could interpret the dynamics of our deep linear networks and relate them to the data. Unfortunately, this rarely happens in practice. Real-world problems often require very deep and nonlinear networks with many hyper-parameters. And ordinarily, these complex networks take hours, if not days, to train.</p>
<p>Let’s recall the training loss curves. There was often a long plateau (where the weights are stuck at a saddle point), followed by a sudden drop. For very deep complex neural nets, such plateaus can last for hours of training, and we often decide to stop the training because we believe it “as good as it gets”! This raises the challenge of whether the network has learned all the “intended” hidden representations. But more importantly, the network might find an illusionary correlation between features that has never seen.</p>
<p>To better understand this, let’s do the next demonstration and exercise.</p>
</div>
<div class="section" id="demonstration-illusory-correlations">
<h2>Demonstration: Illusory Correlations<a class="headerlink" href="#demonstration-illusory-correlations" title="Permalink to this headline">¶</a></h2>
<p>Our original dataset has 4 animals: Canary, Robin, Goldfish, and Tuna. These animals all have bones. Therefore if we include a “has bone” feature, the network would learn it at the second level (i.e. second bump, second mode convergence), when it learns the animal-plants distinction.</p>
<p>What if the dataset has Shark instead of Goldfish. Sharks don’t have bones (their skeletons are made of cartilaginous, which is much lighter than true bone and more flexible). Then we will have a feature which is <em>True</em> (i.e. +1) for Tuna, Robin, and Canary, but <em>False</em> (i.e. -1) for all the plants and the shark! Let’s see what the network does.</p>
<p>First, we add the new feature to the targets. We then start training our LNN and in every epoch, record the network prediction for “sharks having bones”.</p>
<center><img alt="Simple nn graph" src="https://raw.githubusercontent.com/ssnio/statics/main/neuromatch/shark_tree.png" width="600"/></center><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sampling new data from the tree</span>
<span class="n">tree_labels</span><span class="p">,</span> <span class="n">tree_features</span> <span class="o">=</span> <span class="n">generate_hsd</span><span class="p">()</span>

<span class="c1"># replacing Goldfish with Shark</span>
<span class="n">item_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Shark'</span><span class="p">,</span> <span class="s1">'Tuna'</span><span class="p">,</span> <span class="s1">'Robin'</span><span class="p">,</span> <span class="s1">'Canary'</span><span class="p">,</span>
              <span class="s1">'Rose'</span><span class="p">,</span> <span class="s1">'Daisy'</span><span class="p">,</span> <span class="s1">'Pine'</span><span class="p">,</span> <span class="s1">'Oak'</span><span class="p">]</span>

<span class="c1"># index of label to record</span>
<span class="n">illusion_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Shark is the first element</span>

<span class="c1"># the new feature (has bones) vector</span>
<span class="n">new_feature</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">its_label</span> <span class="o">=</span> <span class="s1">'has_bones'</span>

<span class="c1"># adding feature has_bones to the feature array</span>
<span class="n">tree_features</span> <span class="o">=</span> <span class="n">add_feature</span><span class="p">(</span><span class="n">tree_features</span><span class="p">,</span> <span class="n">new_feature</span><span class="p">)</span>

<span class="c1"># plotting</span>
<span class="n">plot_tree_data</span><span class="p">(</span><span class="n">item_names</span><span class="p">,</span> <span class="n">tree_features</span><span class="p">,</span> <span class="n">its_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the new feature shown in the last column of the plot above.</p>
<p>Now we can train the network on the new data, and record the network prediction (output) for Shark (indexed 0) label and “has bone” feature (last feature, indexed -1), during the training.</p>
<p>Here is the snippet from the training loop that keeps track of network prediction for <code class="docutils literal notranslate"><span class="pre">illusory_i</span></code>th label and last (<code class="docutils literal notranslate"><span class="pre">-1</span></code>) feature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pred_ij</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">illusory_i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="id2">
<h3>Make sure you execute this cell to train the network and plot<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to train the network and plot</span>

<span class="c1"># convert (cast) data from np.ndarray to torch.Tensor</span>
<span class="n">label_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">feature_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_features</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="n">feature_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># model instantiation</span>
<span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

<span class="c1"># weights re-initialization</span>
<span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">_</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">ill_predictions</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                                     <span class="n">label_tensor</span><span class="p">,</span>
                                     <span class="n">feature_tensor</span><span class="p">,</span>
                                     <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                                     <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
                                     <span class="n">illusory_i</span><span class="o">=</span><span class="n">illusion_idx</span><span class="p">)</span>

<span class="c1"># a label for the plot</span>
<span class="n">ill_label</span> <span class="o">=</span> <span class="s2">"Prediction for "</span> <span class="o">+</span> <span class="n">item_names</span><span class="p">[</span><span class="n">illusion_idx</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">its_label</span>

<span class="c1"># plotting</span>
<span class="n">plot_ills_sv_twin</span><span class="p">(</span><span class="n">ill_predictions</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">ill_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>It seems that the network starts by learning an “illusory correlation” that sharks have bones, and in later epochs, as it learns deeper representations, it can see (learn) beyond the illusory correlation. This is important to remember that we never presented the network with any data saying that sharks have bones.</p>
</div>
<div class="section" id="video-5-illusory-correlations-explained">
<h3>Video 5: Illusory Correlations Explained<a class="headerlink" href="#video-5-illusory-correlations-explained" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
</div>
</div>
<div class="section" id="exercise-4-illusory-correlations">
<h2>Exercise 4: Illusory Correlations<a class="headerlink" href="#exercise-4-illusory-correlations" title="Permalink to this headline">¶</a></h2>
<p>This exercise is just for you to explore the idea of illusory correlations. Think of medical, natural, or possibly social illusory correlations which can test the learning power of deep linear neural nets.</p>
<p><strong>important notes</strong>: the generated data is independent of tree labels, therefore the names are just for convenience.</p>
<p>Here is our example for <strong>Non-human Living things do not speak</strong>. The lines marked by <code class="docutils literal notranslate"><span class="pre">{edit}</span></code> are for you to change in your example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sampling new data from the tree</span>
<span class="n">tree_labels</span><span class="p">,</span> <span class="n">tree_features</span> <span class="o">=</span> <span class="n">generate_hsd</span><span class="p">()</span>

<span class="c1"># {edit} replacing Canary with Parrot</span>
<span class="n">item_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Goldfish'</span><span class="p">,</span> <span class="s1">'Tuna'</span><span class="p">,</span> <span class="s1">'Robin'</span><span class="p">,</span> <span class="s1">'Parrot'</span><span class="p">,</span>
              <span class="s1">'Rose'</span><span class="p">,</span> <span class="s1">'Daisy'</span><span class="p">,</span> <span class="s1">'Pine'</span><span class="p">,</span> <span class="s1">'Oak'</span><span class="p">]</span>

<span class="c1"># {edit} index of label to record</span>
<span class="n">illusion_idx</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Parrot is the fourth element</span>

<span class="c1"># {edit} the new feature (cannot speak) vector</span>
<span class="n">new_feature</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">its_label</span> <span class="o">=</span> <span class="s1">'cannot_speak'</span>

<span class="c1"># adding feature has_bones to the feature array</span>
<span class="n">tree_features</span> <span class="o">=</span> <span class="n">add_feature</span><span class="p">(</span><span class="n">tree_features</span><span class="p">,</span> <span class="n">new_feature</span><span class="p">)</span>

<span class="c1"># plotting</span>
<span class="n">plot_tree_data</span><span class="p">(</span><span class="n">item_names</span><span class="p">,</span> <span class="n">tree_features</span><span class="p">,</span> <span class="n">its_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Make sure you execute this cell to train the network and plot<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown #### Make sure you execute this cell to train the network and plot</span>

<span class="c1"># convert (cast) data from np.ndarray to torch.Tensor</span>
<span class="n">label_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">feature_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_features</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="n">feature_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># model instantiation</span>
<span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

<span class="c1"># weights re-initialization</span>
<span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">_</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">ill_predictions</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                                     <span class="n">label_tensor</span><span class="p">,</span>
                                     <span class="n">feature_tensor</span><span class="p">,</span>
                                     <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                                     <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
                                     <span class="n">illusory_i</span><span class="o">=</span><span class="n">illusion_idx</span><span class="p">)</span>

<span class="c1"># a label for the plot</span>
<span class="n">ill_label</span> <span class="o">=</span> <span class="s2">"Prediction for "</span> <span class="o">+</span> <span class="n">item_names</span><span class="p">[</span><span class="n">illusion_idx</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">its_label</span>

<span class="c1"># plotting</span>
<span class="n">plot_ills_sv_twin</span><span class="p">(</span><span class="n">ill_predictions</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">ill_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="wrap-up">
<h1>Wrap up<a class="headerlink" href="#wrap-up" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-6-outro">
<h2>Video 6: Outro<a class="headerlink" href="#video-6-outro" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-7-linear-regression">
<h2>Video 7: Linear Regression<a class="headerlink" href="#video-7-linear-regression" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-5-1-linear-regression">
<h2>Section 5.1: Linear Regression<a class="headerlink" href="#section-5-1-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Generally, <em>regression</em> refers to a set of methods for modeling the mapping (relationship) between one (or more) independent variable(s) (i.e. features) and one (or more) dependent variable(s) (i.e. labels). For example, if we want to examine the relative impacts of calendar date, GPS coordinates, and time of the say (the independent variables) on air temperature (the dependent variable). On the other hand, regression can be used for predictive analysis. Thus the independent variables are also called predictors. When the model contains more than one predictor, then the method is called <em>multiple regression</em>, and if it contains more than one dependent variable called <em>multivariate regression</em>. Regression problems pop up whenever we want to predict a numerical (usually continuous) value.</p>
<p>The independent variables are collected in vector <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^M\)</span>, where <span class="math notranslate nohighlight">\(M\)</span> denotes the number of independent variables, while the dependent variables are collected in vector <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^N\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> denotes the number of independent variables. And the mapping between them is represented by the weight matrix <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{N \times M}\)</span> and a bias vector <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{N}\)</span> (generalizing to affine mappings).</p>
<p>The multivariate regression model can be written as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d3c16416-f12e-4bf3-978b-a210a124fbde">
<span class="eqno">(6)<a class="headerlink" href="#equation-d3c16416-f12e-4bf3-978b-a210a124fbde" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{y} = \mathbf{W} ~ \mathbf{x} + \mathbf{b}
\end{equation}\]</div>
<p>or it can be written in matrix format as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e6052d6a-4f74-43db-b8cb-8d64fc523f4c">
<span class="eqno">(7)<a class="headerlink" href="#equation-e6052d6a-4f74-43db-b8cb-8d64fc523f4c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{N} \\ \end{bmatrix} = \begin{bmatrix} w_{1,1} &amp; w_{1,2} &amp; \dots &amp; w_{1,M} \\ w_{2,1} &amp; w_{2,2} &amp; \dots &amp; w_{2,M} \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\ w_{N,1} &amp; w_{N,2} &amp; \dots &amp; w_{N,M} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{M} \\ \end{bmatrix} + \begin{bmatrix} b_{1} \\ b_{2} \\ \vdots \\b_{N} \\ \end{bmatrix}
\end{equation}\]</div>
</div>
<div class="section" id="section-5-2-vectorized-regression">
<h2>Section 5.2: Vectorized regression<a class="headerlink" href="#section-5-2-vectorized-regression" title="Permalink to this headline">¶</a></h2>
<p>Linear regression can be simply extended to multi-samples (<span class="math notranslate nohighlight">\(D\)</span>) input-output mapping, which we can collect in a matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{M \times D}\)</span>, sometimes called the design matrix. The sample dimension also shows up in the output matrix <span class="math notranslate nohighlight">\(\mathbf{Y} \in \mathbb{R}^{N \times D}\)</span>. Thus, linear regression takes the following form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a72b81bd-d005-48d7-aa33-9066cbbe1ccd">
<span class="eqno">(8)<a class="headerlink" href="#equation-a72b81bd-d005-48d7-aa33-9066cbbe1ccd" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{Y} = \mathbf{W} ~ \mathbf{X} + \mathbf{b}
\end{equation}\]</div>
<p>where matrix <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{N \times M}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{N}\)</span> (broudcasted over sample dimension) are the desired parameters to find.</p>
</div>
<div class="section" id="section-5-3-analytical-linear-regression">
<h2>Section 5.3: Analytical Linear Regression<a class="headerlink" href="#section-5-3-analytical-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Linear regression is a relatively simple optimization problem. Unlike most other models that we will see in this course, linear regression for mean squared loss can be solved analytically.</p>
<p>For <span class="math notranslate nohighlight">\(D\)</span> samples (batch size), <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{M \times D}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{Y} \in \mathbb{R}^{N \times D}\)</span>, the goal of linear regression is to find <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{N \times M}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \mathbf{W} ~ \mathbf{X} \]</div>
<p>Given the Squared Error loss function, we have:</p>
<div class="amsmath math notranslate nohighlight" id="equation-53a55b8d-5b25-4704-81ab-3b82c23e2a28">
<span class="eqno">(9)<a class="headerlink" href="#equation-53a55b8d-5b25-4704-81ab-3b82c23e2a28" title="Permalink to this equation">¶</a></span>\[\begin{equation}
Loss(\mathbf{W}) = ||\mathbf{Y} - \mathbf{W} ~ \mathbf{X}||^2
\end{equation}\]</div>
<p>So, using matrix notation, the optimization problem is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4b26bad9-a5ea-4394-933a-1d4f8fa631cc">
<span class="eqno">(10)<a class="headerlink" href="#equation-4b26bad9-a5ea-4394-933a-1d4f8fa631cc" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbf{W^{*}} &amp;= \underset{\mathbf{W}}{\mathrm{argmin}} \left( Loss (\mathbf{W})  \right) \\
 &amp;= \underset{\mathbf{W}}{\mathrm{argmin}} \left( ||\mathbf{Y} - \mathbf{W} ~ \mathbf{X}||^2  \right) \\
&amp;= \underset{\mathbf{W}}{\mathrm{argmin}} \left( \left( \mathbf{Y} - \mathbf{W} ~ \mathbf{X}\right)^{\top} \left( \mathbf{Y} - \mathbf{W} ~ \mathbf{X}\right) \right)
\end{align}\]</div>
<p>To solve the minimization problem, we can simply set the derivative of the loss with respect to <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> to zero.</p>
<div class="amsmath math notranslate nohighlight" id="equation-a4880593-4dff-499d-8697-de25b059b6e0">
<span class="eqno">(11)<a class="headerlink" href="#equation-a4880593-4dff-499d-8697-de25b059b6e0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\dfrac{\partial Loss}{\partial \mathbf{W}} = 0
\end{equation}\]</div>
<p>Assuming that <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{X}^{\top}\)</span> is full-rank, and thus it is invertible we can write:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b8916261-09b2-49b7-8e56-54171d03e8ca">
<span class="eqno">(12)<a class="headerlink" href="#equation-b8916261-09b2-49b7-8e56-54171d03e8ca" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{W}^{\mathbf{*}} = \mathbf{Y} \mathbf{X}^{\top} \left( \mathbf{X}  \mathbf{X}^{\top} \right) ^{-1}
\end{equation}\]</div>
<div class="section" id="coding-exercise-5-3-1-analytical-solution-to-lr">
<h3>Coding Exercise 5.3.1: Analytical solution to LR<a class="headerlink" href="#coding-exercise-5-3-1-analytical-solution-to-lr" title="Permalink to this headline">¶</a></h3>
<p>Complete the function <code class="docutils literal notranslate"><span class="pre">linear_regression</span></code> for finding the analytical solution to linear regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
  <span class="sd">"""Analytical Linear regression</span>

<span class="sd">  Args:</span>
<span class="sd">    X (np.ndarray): design matrix</span>
<span class="sd">    Y (np.ndarray): target ouputs</span>

<span class="sd">  return:</span>
<span class="sd">    np.ndarray: estimated weights (mapping)</span>
<span class="sd">  """</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
  <span class="n">M</span><span class="p">,</span> <span class="n">Dx</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">N</span><span class="p">,</span> <span class="n">Dy</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">assert</span> <span class="n">Dx</span> <span class="o">==</span> <span class="n">Dy</span>
  <span class="c1">#################################################</span>
  <span class="c1">## Complete the linear_regression_exercise function</span>
  <span class="c1"># Complete the function and remove or comment the line below</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Linear Regression `linear_regression`"</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="n">W</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">W</span>


<span class="n">W_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">37</span><span class="p">)</span>  <span class="c1"># 37 samples</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">37</span><span class="p">))</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">W_true</span> <span class="o">@</span> <span class="n">X_train</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1">## Uncomment and run</span>
<span class="c1"># W_estimate = linear_regression(X_train, Y_train)</span>
<span class="c1"># print("True weights:\n", W_true)</span>
<span class="c1"># print("\nEstimated weights:\n", np.round(W_estimate, 1))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D2_LinearDeepLearning/solutions/W1D2_Tutorial3_Solution_63a46f9b.py"><em>Click for solution</em></a></p>
</div>
</div>
<div class="section" id="demonstration-linear-regression-vs-dlnn">
<h2>Demonstration: Linear Regression vs. DLNN<a class="headerlink" href="#demonstration-linear-regression-vs-dlnn" title="Permalink to this headline">¶</a></h2>
<p>A linear neural network with NO hidden layer is very similar to linear regression in its core. We also know that no matter how many hidden layers a linear network has, it can be compressed to linear regression (no hidden layers).</p>
<p>In this demonstration, we use the hierarchically structured data to:</p>
<ul class="simple">
<li><p>analytically find the mapping between features and labels</p></li>
<li><p>train a zero-depth LNN to find the mapping</p></li>
<li><p>compare them to the <span class="math notranslate nohighlight">\(W_{tot}\)</span> from the already trained deep LNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sampling new data from the tree</span>
<span class="n">tree_labels</span><span class="p">,</span> <span class="n">tree_features</span> <span class="o">=</span> <span class="n">generate_hsd</span><span class="p">()</span>

<span class="c1"># convert (cast) data from np.ndarray to torch.Tensor</span>
<span class="n">label_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">feature_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tree_features</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculating the W_tot for deep network (already trained model)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">100.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_hidden</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># hidden neurons</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

<span class="c1"># model instantiation</span>
<span class="n">dlnn_model</span> <span class="o">=</span> <span class="n">LNNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>

<span class="c1"># weights re-initialization</span>
<span class="n">initializer_</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">modes</span><span class="p">,</span> <span class="n">rsms</span><span class="p">,</span> <span class="n">ills</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dlnn_model</span><span class="p">,</span>
                                  <span class="n">label_tensor</span><span class="p">,</span>
                                  <span class="n">feature_tensor</span><span class="p">,</span>
                                  <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                                  <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">deep_W_tot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dim_input</span><span class="p">)</span>
<span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">dlnn_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">deep_W_tot</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">@</span> <span class="n">deep_W_tot</span>
<span class="n">deep_W_tot</span> <span class="o">=</span> <span class="n">deep_W_tot</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># analytically estimation of weights</span>
<span class="c1"># our data is batch first dimension, so we need to transpose our data</span>
<span class="n">analytical_weights</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">tree_labels</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tree_features</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LRNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""A Linear Neural Net with ZERO hidden layer (LR net)</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">      in_dim (int): input dimension</span>
<span class="sd">      hid_dim (int): hidden dimension</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">      x (torch.Tensor): input tensor</span>
<span class="sd">    """</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># output (prediction)</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1000.0</span>  <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># initialization scale</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># number of epochs</span>
<span class="n">dim_input</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># input dimension = `label_tensor.size(1)`</span>
<span class="n">dim_output</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># output dimension = `feature_tensor.size(1)`</span>

<span class="c1"># model instantiation</span>
<span class="n">LR_model</span> <span class="o">=</span> <span class="n">LRNet</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_output</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">LR_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>  <span class="c1"># loss records</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>  <span class="c1"># training loop</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">predictions</span> <span class="o">=</span> <span class="n">LR_model</span><span class="p">(</span><span class="n">label_tensor</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">feature_tensor</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
  <span class="n">losses</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># trained weights from zero_depth_model</span>
<span class="n">LR_model_weights</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">LR_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="s2">"Training loss for zero depth LNN"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"r"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"The final weights from all methods are approximately equal?! "</span>
<span class="s2">"</span><span class="si">{}</span><span class="s2">!"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
  <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">analytical_weights</span><span class="p">,</span> <span class="n">LR_model_weights</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">)</span> <span class="ow">and</span> \
   <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">analytical_weights</span><span class="p">,</span> <span class="n">deep_W_tot</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">))</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As you may have guessed, they all arrive at the same results but through very different paths.</p>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D2_LinearDeepLearning/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="W1D2_Tutorial2.html" id="prev-link" title="previous page">Tutorial 2: Learning Hyperparameters</a>
<a class="right-next" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html" id="next-link" title="next page">Multi Layer Perceptrons</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>